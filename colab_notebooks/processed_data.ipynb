{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":11289,"status":"ok","timestamp":1702070564815,"user":{"displayName":"Oussama JLASSI","userId":"16366124337863282764"},"user_tz":300},"id":"zIvuSi9P2nOq"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, LSTM\n","from keras.utils import to_categorical\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":233,"status":"ok","timestamp":1702070579182,"user":{"displayName":"Oussama JLASSI","userId":"16366124337863282764"},"user_tz":300},"id":"sivAStPhm-g8"},"outputs":[],"source":["def subject_wise_split(\n","    x, y, participant, subject_wise=True, test_size=0.10, random_state=42\n","):\n","    \"\"\"Split data into train and test sets via an inter-subject scheme, see:\n","    Shah, V., Flood, M. W., Grimm, B., & Dixon, P. C. (2022). Generalizability of deep learning models for predicting outdoor irregular walking surfaces.\n","    Journal of Biomechanics, 139, 111159. https://doi.org/10.1016/j.jbiomech.2022.111159\n","\n","    Arguments:\n","        x: nd.array, feature space\n","        y: nd.array, label class\n","        participant: nd.array, participant associated with each row in x and y\n","        subject_wise: bool, choices {True, False}, default = True. True = subject-wise split approach, False random-split\n","        test_size: float, number between 0 and 1. Default = 0.10. percentage spilt for test set.\n","        random_state: int. default = 42. Seed selector for numpy random number generator.\n","    Returns:\n","        x_train: nd.array, train set for feature space\n","        x_test: nd.array, test set for feature space\n","        y_train: nd.array, train set label class\n","        y_test: nd.array, test set label class\n","        subject_train: nd.array[string], train set for participants by row of input data\n","        subjects_test: nd.array[string[, test set for participants by row of input data\n","    \"\"\"\n","    if type(participant) == list:\n","        participant = np.asarray(participant, dtype=np.float32)\n","\n","    np.random.seed(random_state)\n","    if subject_wise:\n","        # Extract unique participants\n","        uniq_parti = np.unique(participant)\n","        # Calculate the number of participants for the test set\n","        num = np.round(uniq_parti.shape[0] * test_size).astype(\"int64\")\n","        np.random.shuffle(uniq_parti)\n","        extract = uniq_parti[0:num]\n","        test_index = np.array([], dtype=\"int64\")\n","        for j in extract:\n","            test_index = np.append(test_index, np.where(participant == j)[0])\n","        # Remove test set indices from all indices to get train set indices\n","        train_index = np.delete(np.arange(len(participant)), test_index)\n","        # Shuffle the train and test indices\n","        np.random.shuffle(test_index)\n","        np.random.shuffle(train_index)\n","\n","    else:\n","        index = np.arange(len(participant)).astype(\"int64\")\n","        np.random.shuffle(index)\n","        num = np.round(participant.shape[0] * test_size).astype(\"int64\")\n","        # Select train and test set indices\n","        test_index = index[0:num]\n","        train_index = index[num:]\n","\n","    # Extract train and test sets based on the computed indices\n","    x_train = x[train_index]\n","    x_test = x[test_index]\n","    y_train = y[train_index]\n","    y_test = y[test_index]\n","    subject_train = participant[train_index]\n","    subject_test = participant[test_index]\n","\n","    return x_train, x_test, y_train, y_test, subject_train, subject_test"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3297,"status":"ok","timestamp":1702070585300,"user":{"displayName":"Oussama JLASSI","userId":"16366124337863282764"},"user_tz":300},"id":"yU04oIia3r7w"},"outputs":[],"source":["# Specify the correct path to the processed data files\n","ap = pd.read_csv(\"path_to_GRF_F_AP_PRO_right.csv\")\n","ml = pd.read_csv(\"path_to_GRF_F_ML_PRO_right.csv\")\n","v = pd.read_csv(\"path_to_GRF_F_V_PRO_right.csv\")\n","\n","# Separate the output 'SEX' column\n","y = ap['SEX']\n","\n","# Separate the column 'SUBJECT_ID' to be used for splitting\n","subject = ap['SUBJECT_ID']\n","\n","# Drop the first four and last 2 columns from each DataFrame\n","X_data1 = ap.iloc[:, 4:-2]\n","X_data2 = ml.iloc[:, 4:-2]\n","X_data3 = v.iloc[:, 4:-2]"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1702070585302,"user":{"displayName":"Oussama JLASSI","userId":"16366124337863282764"},"user_tz":300},"id":"NXIqGWCWHMGk"},"outputs":[],"source":["# Stack the data to make a tensor\n","X = np.stack([X_data1.values, X_data2.values, X_data3.values], axis=-1)\n","\n","# Splitting the data into train and test sets\n","X_train, X_test, y_train, y_test, sub_train, sub_test = subject_wise_split(X, y, subject, subject_wise=True, test_size=0.2, random_state=42)\n","\n","# One hot encode labels\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNEuPmW58Z8n"},"outputs":[],"source":["print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Av78V-20NdbI"},"source":["# **CNN model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAJ-tt-Y8Er9"},"outputs":[],"source":["# Initialize empty lists to store the results\n","results_f1 = []\n","results_accuracy = []\n","\n","for i in range(10):\n","  print(\"Iteration\", i+1)\n","  # Define the CNN model\n","  model = Sequential()\n","\n","  model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Conv1D(64, kernel_size=3, activation='relu'))\n","  model.add(MaxPooling1D(pool_size=2))\n","  model.add(Flatten())\n","\n","  # Dense layers for classification\n","  model.add(Dense(128, activation='relu'))\n","  model.add(Dense(2, activation='softmax'))  # 2 is the number of classes\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","  # # Print a summary of the model architecture\n","  # model.summary()\n","\n","  y_test_class = np.argmax(y_test, axis=1)\n","\n","  # Train the model\n","  history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=False)\n","\n","  # Generate predictions on the test set\n","  y_pred = model.predict(X_test)\n","  y_pred_class = np.argmax(y_pred, axis=1)\n","\n","  # Calculate classification metrics (e.g., precision, recall, f1-score)\n","  report = classification_report(y_test_class, y_pred_class, output_dict=True)\n","  results_f1.append(report[\"weighted avg\"][\"f1-score\"])\n","  results_accuracy.append(report[\"accuracy\"])\n","\n","# Calculate the mean and standard deviation of the f1 score and accuracy\n","mean_f1 = np.mean(results_f1)\n","std_f1 = np.std(results_f1)\n","mean_accuracy = np.mean(results_accuracy)\n","std_accuracy = np.std(results_accuracy)\n","\n","# Print results\n","print(\"Mean f1 score:\", mean_f1)\n","print(\"Standard deviation of the f1 score:\", std_f1)\n","print(\"Mean accuracy:\", mean_accuracy)\n","print(\"Standard deviation of the accuracy:\", std_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"n_FTmbRJOoNB"},"source":["# **LSTM model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-DiHSWjOq9e"},"outputs":[],"source":["# Initialize empty lists to store the results\n","results_f1 = []\n","results_accuracy = []\n","\n","for i in range(10):\n","  print(\"Iteration\", i+1)\n","  # Define the LSTM model\n","  model = Sequential()\n","  model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n","  model.add(Dense(units=2, activation='softmax'))  # Assuming 2 classes for classification\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","  # # Print a summary of the model architecture\n","  # model.summary()\n","\n","  y_test_class = np.argmax(y_test, axis=1)\n","\n","  # Train the model\n","  history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=False)\n","\n","  # Generate predictions on the test set\n","  y_pred = model.predict(X_test)\n","  y_pred_class = np.argmax(y_pred, axis=1)\n","\n","  # Calculate classification metrics (e.g., precision, recall, f1-score)\n","  report = classification_report(y_test_class, y_pred_class, output_dict=True)\n","  results_f1.append(report[\"weighted avg\"][\"f1-score\"])\n","  results_accuracy.append(report[\"accuracy\"])\n","\n","# Calculate the mean and standard deviation of the f1 score and accuracy\n","mean_f1 = np.mean(results_f1)\n","std_f1 = np.std(results_f1)\n","mean_accuracy = np.mean(results_accuracy)\n","std_accuracy = np.std(results_accuracy)\n","\n","# Print results\n","print(\"Mean f1 score:\", mean_f1)\n","print(\"Standard deviation of the f1 score:\", std_f1)\n","print(\"Mean accuracy:\", mean_accuracy)\n","print(\"Standard deviation of the accuracy:\", std_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfaJTpQ_B5Bb"},"outputs":[],"source":["# Access training and validation loss from the history object\n","training_loss = history.history['loss']\n","validation_loss = history.history['val_loss']\n","\n","# Create a line plot for training and validation loss\n","epochs = range(1, len(training_loss) + 1)\n","plt.plot(epochs, training_loss, 'b', label='Training Loss')\n","plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPa8K6c8du3x2lfGKN0vONP","mount_file_id":"1jJI5kVFJKNR-R3ww5UWBN8puEPlNRCg5","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
